# -*- coding: utf-8 -*-
"""CS599Lab4.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Cfoet-RKJo6FUSMUJZ6e7wzeF2cYeRjl
"""

!pip install tensorflow matplotlib

import numpy as np, random
import tensorflow as tf
print(tf.__version__)  # should be >= 2.x
import pathlib
import os
from PIL import Image
from tensorflow.keras.layers import Layer
from tensorflow.keras.layers import RNN, Dense, Input
from tensorflow.keras.models import Model
from tensorflow.keras.layers import Lambda
import matplotlib.pyplot as plt

def set_seed(seed):
    tf.random.set_seed(seed)
    np.random.seed(seed)
    random.seed(seed)

!wget http://yaroslavvb.com/upload/notMNIST/notMNIST_small.tar.gz
!mkdir -p ./notMNIST_small
!tar -xzf notMNIST_small.tar.gz -C ./notMNIST_small

def remove_corrupt_images(root_dir):
    removed = 0
    for class_dir in os.listdir(root_dir):
        class_path = os.path.join(root_dir, class_dir)
        if not os.path.isdir(class_path):
            continue
        for fname in os.listdir(class_path):
            fpath = os.path.join(class_path, fname)
            if not os.path.isfile(fpath):
                continue
            try:
                with Image.open(fpath) as img:
                    img.verify()
            except Exception:
                print("Deleting:", fpath)
                os.remove(fpath)
                removed += 1
    print(f"Removed {removed} corrupt images.")

remove_corrupt_images("notMNIST_small/notMNIST_small")

data_dir = "notMNIST_small/notMNIST_small"
train_ds = tf.keras.utils.image_dataset_from_directory(
    data_dir,
    validation_split=0.2,
    subset='training',
    seed=123,
    image_size=(28, 28),
    batch_size=64,
    color_mode='grayscale',
    label_mode='int'
)

val_ds = tf.keras.utils.image_dataset_from_directory(
    data_dir,
    validation_split=0.2,
    subset='validation',
    seed=123,
    image_size=(28, 28),
    batch_size=64,
    color_mode='grayscale',
    label_mode='int'
)
def preprocess(img, lbl):
    img = tf.cast(img, tf.float32) / 255.0
    lbl = tf.one_hot(lbl, depth=10)
    return img, lbl

train_ds = train_ds.map(preprocess).prefetch(tf.data.AUTOTUNE)
val_ds   = val_ds.map(preprocess).prefetch(tf.data.AUTOTUNE)

class BaseRNNCell(Layer):
    def __init__(self, hidden_size, **kwargs):
        super().__init__(**kwargs)
        self.hidden_size = hidden_size

    @property
    def state_size(self):
        # Keras will allocate a state tensor of shape (batch, hidden_size)
        return self.hidden_size

    def build(self, input_shape):
        # input_shape = (batch, feature_dim)
        # We don't create weights here—we let subclasses do that.
        super().build(input_shape)

    def call(self, inputs, states):
        # inputs: (batch, feature_dim)
        # states: list of previous states; here states[0] is (batch, hidden_size)
        raise NotImplementedError("Must implement in subclass")

class GRUCell(BaseRNNCell):
    def build(self, input_shape):
        # input_shape[-1] = D
        D = input_shape[-1]
        H = self.hidden_size
        total_dim = D + H  # we'll concatenate [h_prev, x]

        # 1) Update gate parameters: zₜ = sigmoid([h_prev, x]·Wz + bz)
        self.Wz = self.add_weight(
            name="Wz", shape=[total_dim, H], initializer="glorot_uniform")
        self.bz = self.add_weight(
            name="bz", shape=[H], initializer="zeros")

        # 2) Reset gate parameters: rₜ = sigmoid([h_prev, x]·Wr + br)
        self.Wr = self.add_weight(
            name="Wr", shape=[total_dim, H], initializer="glorot_uniform")
        self.br = self.add_weight(
            name="br", shape=[H], initializer="zeros")

        # 3) Candidate: ŧhₜ = tanh([r⊙h_prev, x]·Ws + bs)
        self.Ws = self.add_weight(
            name="Ws", shape=[total_dim, H], initializer="glorot_uniform")
        self.bs = self.add_weight(
            name="bs", shape=[H], initializer="zeros")

        super().build(input_shape)  # finalize build

    def call(self, inputs, states):
        # inputs: (batch, D), states[0]=h_prev: (batch, H)
        h_prev = states[0]

        # 1) Concatenate previous state & input
        concat_hx = tf.concat([h_prev, inputs], axis=-1)  # → (batch, H+D)

        # 2) Compute update & reset gates
        z = tf.sigmoid(tf.matmul(concat_hx, self.Wz) + self.bz)
        r = tf.sigmoid(tf.matmul(concat_hx, self.Wr) + self.br)

        # 3) Compute candidate hidden state
        gated_h = r * h_prev                             # (batch, H)
        concat_candidate = tf.concat([gated_h, inputs], axis=-1)
        h_tilde = tf.tanh(tf.matmul(concat_candidate, self.Ws) + self.bs)

        # 4) Final new hidden state
        new_h = (1 - z) * h_prev + z * h_tilde            # (batch, H)

        # Return output (we choose to output the hidden state) and new state
        return new_h, [new_h]

class MGUCell(BaseRNNCell):
    def build(self, input_shape):
        D = input_shape[-1]
        H = self.hidden_size
        total_dim = D + H

        # 1) Single gate fₜ
        self.Wf = self.add_weight(
            name="Wf", shape=[total_dim, H], initializer="glorot_uniform")
        self.bf = self.add_weight(
            name="bf", shape=[H], initializer="zeros")

        # 2) Candidate hidden state (same as GRU’s Ws, bs)
        self.Ws = self.add_weight(
            name="Ws", shape=[total_dim, H], initializer="glorot_uniform")
        self.bs = self.add_weight(
            name="bs", shape=[H], initializer="zeros")

        super().build(input_shape)

    def call(self, inputs, states):
        h_prev = states[0]

        # 1) Concatenate [h_prev; x]
        concat_hx = tf.concat([h_prev, inputs], axis=-1)

        # 2) Compute gate fₜ
        f = tf.sigmoid(tf.matmul(concat_hx, self.Wf) + self.bf)

        # 3) Candidate uses f as reset
        gated_h = f * h_prev
        concat_candidate = tf.concat([gated_h, inputs], axis=-1)
        h_tilde = tf.tanh(tf.matmul(concat_candidate, self.Ws) + self.bs)

        # 4) New state
        new_h = (1 - f) * h_prev + f * h_tilde

        return new_h, [new_h]

def build_rnn_model(cell_cls, hidden_size, num_layers, num_classes=10):
    # instantiate each layer’s cell
    cells = [cell_cls(hidden_size) for _ in range(num_layers)]
    rnn_stack = RNN(cells, return_sequences=False)

    inp = Input(shape=(28, 28, 1))
    x = tf.keras.layers.Reshape((28, 28))(inp)

    # RNN + final Dense layer
    h_final = rnn_stack(x)
    logits = Dense(num_classes)(h_final)

    return Model(inputs=inp, outputs=logits)

def compile_and_train(model, train_ds, test_ds, epochs=20):
    class EpochLogger(tf.keras.callbacks.Callback):
        def on_epoch_end(self, epoch, logs=None):
            if (epoch + 1) % 5 == 0 or (epoch + 1) == epochs:
                acc = logs.get("val_accuracy", 0)
                err = 1 - acc
                print(f"Epoch {epoch + 1:02d} - Val Accuracy: {acc:.4f} - Classification Error: {err:.4f}")

    model.compile(
        optimizer=tf.keras.optimizers.Adam(learning_rate=1e-3),
        loss=tf.keras.losses.CategoricalCrossentropy(from_logits=True),
        metrics=["accuracy"]
    )
    history = model.fit(
        train_ds,
        validation_data=test_ds,
        epochs=epochs,
        callbacks=[EpochLogger()],
        verbose=0
    )
    return history

results = {}

selected_configs = [
    (1, 128),
    (2, 256),
    (4, 128),
    (2, 512)
]

for cell_name, cell_cls in [("GRU", GRUCell), ("MGU", MGUCell)]:
    for num_layers, hidden_size in selected_configs:
        config_key = f"{cell_name}_{num_layers}l_{hidden_size}h"
        results[config_key] = []

        print("\n==========================================")
        print(f"Model: {cell_name}")
        print(f"Hidden Size: {hidden_size}")
        print(f"Number of Layers: {num_layers}")
        print(f"Configuration Key: {config_key}")
        print("==========================================")

        for trial in range(3):
            seed = 1000 + trial
            set_seed(seed)
            print(f"Running Trial {trial + 1}/3 (Seed: {seed})...")

            model = build_rnn_model(cell_cls, hidden_size, num_layers)
            history = compile_and_train(model, train_ds, val_ds, epochs=20)

            val_acc = history.history["val_accuracy"][-1]
            val_err = 1.0 - val_acc
            print(f"Trial {trial + 1} complete - Final Validation Accuracy: {val_acc:.4f} - Classification Error: {val_err:.4f}")

            results[config_key].append(history.history)

import matplotlib.pyplot as plt
import numpy as np

def plot_metric(results, metric="accuracy", val=True):
    plt.figure(figsize=(8,5))
    for key, runs in results.items():
        # runs: list of 3 history dicts
        vals = np.array([r[f"val_{metric}"] for r in runs])  # shape (3, epochs)
        mean = vals.mean(axis=0)
        std  = vals.std(axis=0)
        epochs = np.arange(1, len(mean)+1)

        # Shade std region
        plt.fill_between(epochs, mean-std, mean+std, alpha=0.2)
        # Plot mean curve
        plt.plot(epochs, mean, label=key)

    plt.xlabel("Epoch")
    plt.ylabel(f"Validation {metric.capitalize()}")
    plt.legend(fontsize="small", ncol=2)
    plt.title(f"Mean ± Std of val_{metric} over Trials")
    plt.show()

# Example: plot validation accuracy
plot_metric(results, metric="accuracy")

def plot_train_val_metric(results, metric="accuracy"):
    plt.figure(figsize=(10, 6))

    for key, runs in results.items():
        train_vals = np.array([r[metric] for r in runs])
        val_vals   = np.array([r[f"val_{metric}"] for r in runs])

        train_mean, train_std = train_vals.mean(axis=0), train_vals.std(axis=0)
        val_mean, val_std     = val_vals.mean(axis=0), val_vals.std(axis=0)

        epochs = np.arange(1, len(train_mean) + 1)

        # plot training curve
        plt.plot(epochs, train_mean, linestyle="--", label=f"{key} Train")
        plt.fill_between(epochs, train_mean - train_std, train_mean + train_std, alpha=0.1)

        # plot validation curve
        plt.plot(epochs, val_mean, label=f"{key} Val")
        plt.fill_between(epochs, val_mean - val_std, val_mean + val_std, alpha=0.2)

    plt.xlabel("Epoch")
    plt.ylabel(metric.capitalize())
    plt.title(f"Train vs Validation {metric.capitalize()} (Mean ± Std)")
    plt.legend(fontsize="small", ncol=2)
    plt.grid(True)
    plt.tight_layout()
    plt.show()

plot_train_val_metric(results, metric="accuracy")
plot_train_val_metric(results, metric="loss")